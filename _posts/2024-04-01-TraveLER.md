---
layout: post
title:  TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering (EMNLP 2024) 
date:   2024-04-01 17:24:13 -0700
categories: publications
---
# TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering (EMNLP 2024) 
*Published at EMNLP 2024, Main Conference*
Recently, image-based Large Multimodal Models (LMMs) have made significant progress in video question-answering (VideoQA) using a frame-wise approach by leveraging large-scale pretraining in a zero-shot manner. Nevertheless, these models need to be capable of finding relevant information, extracting it, and answering the question simultaneously. Currently, existing methods perform all of these steps in a single pass without being able to adapt if insufficient or incorrect information is collected. To overcome this, we introduce a modular multi-LMM agent framework based on several agents with different roles, instructed by a Planner agent that updates its instructions using shared feedback from the other agents. Specifically, we propose TraveLER, a method that can create a plan to "Traverse" through the video, ask questions about individual frames to "Locate" and store key information, and then "Evaluate" if there is enough information to answer the question. Finally, if there is not enough information, our method is able to "Replan" based on its collected knowledge. Through extensive experiments, we find that the proposed TraveLER approach improves performance on several VideoQA benchmarks without the need to fine-tune on specific datasets. 

[Project Page](https://traveler-framework.github.io/)
[Paper Link](https://arxiv.org/abs/2404.01476)
